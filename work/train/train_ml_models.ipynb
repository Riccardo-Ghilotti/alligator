{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4d29e1c-80c9-406b-b71f-aa76fedc1ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6207259 entries, 0 to 6207258\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Dtype  \n",
      "---  ------        -----  \n",
      " 0   tableName     object \n",
      " 1   ed            float64\n",
      " 2   jaccard       float64\n",
      " 3   jaccardNgram  float64\n",
      " 4   p_subj_ne     float64\n",
      " 5   p_subj_lit    float64\n",
      " 6   p_obj_ne      float64\n",
      " 7   desc          float64\n",
      " 8   descNgram     float64\n",
      " 9   target        int64  \n",
      "dtypes: float64(8), int64(1), object(1)\n",
      "memory usage: 473.6+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train = pd.read_csv('../data/final_ml/final_ml.csv')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a6c10eb-cf92-4535-ab40-0088a70fe5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 2, 'b': 3, 'z': 1, 'c': 3, 'f': 9}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = {\"a\": 2, \"b\": 3, \"z\": 1, \"c\": 3}\n",
    "t[\"f\"] = 9\n",
    "{e:t[e] for e in t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df785da7-9f0c-49af-b93e-34bfa8454202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl (21.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.22.4)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (63.2.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.30.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.16.0-py2.py3-none-any.whl (177 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.8/177.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.0)\n",
      "Installing collected packages: tensorboard-plugin-wit, pyasn1, libclang, flatbuffers, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, pyasn1-modules, protobuf, opt-einsum, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 flatbuffers-23.1.21 gast-0.4.0 google-auth-2.16.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.1 keras-2.11.0 libclang-15.0.6.1 markdown-3.4.1 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.30.0 termcolor-2.2.0 werkzeug-2.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8e2633-ed6a-4d86-be52-3624bcdeffaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.10.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.1.1)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.8.1)\n",
      "Installing collected packages: joblib, imbalanced-learn\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "Successfully installed imbalanced-learn-0.10.1 joblib-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebfadc48-4291-449f-9e73-945f6a240bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tableName</th>\n",
       "      <th>ntoken</th>\n",
       "      <th>popularity</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>es_score</th>\n",
       "      <th>es_diff_score</th>\n",
       "      <th>ed</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>jaccardNgram</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>p_subj_ne</th>\n",
       "      <th>p_subj_lit</th>\n",
       "      <th>p_obj_ne</th>\n",
       "      <th>desc</th>\n",
       "      <th>descNgram</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99070098_0_2074872741302696997</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.02</td>\n",
       "      <td>37.54</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99070098_0_2074872741302696997</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0.01</td>\n",
       "      <td>37.89</td>\n",
       "      <td>0.009</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99070098_0_2074872741302696997</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.07</td>\n",
       "      <td>37.09</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99070098_0_2074872741302696997</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>72.45</td>\n",
       "      <td>0.037</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99070098_0_2074872741302696997</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>0.75</td>\n",
       "      <td>35.78</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920200</th>\n",
       "      <td>A2E9MHNH</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.02</td>\n",
       "      <td>41.65</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920201</th>\n",
       "      <td>A2E9MHNH</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.15</td>\n",
       "      <td>40.59</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920202</th>\n",
       "      <td>A2E9MHNH</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>0.02</td>\n",
       "      <td>10.08</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920203</th>\n",
       "      <td>A2E9MHNH</td>\n",
       "      <td>1</td>\n",
       "      <td>117</td>\n",
       "      <td>0.43</td>\n",
       "      <td>9.68</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920204</th>\n",
       "      <td>A2E9MHNH</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.66</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2960891 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             tableName  ntoken  popularity  pos_score  \\\n",
       "0       99070098_0_2074872741302696997       1           9       0.02   \n",
       "1       99070098_0_2074872741302696997       1          47       0.01   \n",
       "2       99070098_0_2074872741302696997       1          41       0.07   \n",
       "3       99070098_0_2074872741302696997       3           5       0.01   \n",
       "4       99070098_0_2074872741302696997       3          46       0.75   \n",
       "...                                ...     ...         ...        ...   \n",
       "920200                        A2E9MHNH       1          30       0.02   \n",
       "920201                        A2E9MHNH       1          18       0.15   \n",
       "920202                        A2E9MHNH       1         103       0.02   \n",
       "920203                        A2E9MHNH       1         117       0.43   \n",
       "920204                        A2E9MHNH       1          87       0.49   \n",
       "\n",
       "        es_score  es_diff_score    ed  jaccard  jaccardNgram  \\\n",
       "0          37.54          0.003  1.00      1.0          1.00   \n",
       "1          37.89          0.009  1.00      1.0          1.00   \n",
       "2          37.09          0.002  1.00      1.0          1.00   \n",
       "3          72.45          0.037  1.00      1.0          1.00   \n",
       "4          35.78          0.000  0.22      0.4          0.11   \n",
       "...          ...            ...   ...      ...           ...   \n",
       "920200     41.65          0.004  1.00      1.0          1.00   \n",
       "920201     40.59          0.002  0.38      0.5          0.38   \n",
       "920202     10.08          0.000  0.90      0.0          0.75   \n",
       "920203      9.68          0.001  0.40      0.0          0.31   \n",
       "920204      9.66          0.002  0.64      0.0          0.44   \n",
       "\n",
       "        cosine_similarity  p_subj_ne  p_subj_lit  p_obj_ne  desc  descNgram  \\\n",
       "0                    1.00        0.0       0.000       0.0   0.0        0.0   \n",
       "1                    1.00        0.0       0.000       0.0   0.0        0.0   \n",
       "2                    1.00        0.0       0.619       0.0   0.0        0.0   \n",
       "3                    1.00        0.0       0.603       0.0   0.0        0.0   \n",
       "4                    0.56        0.0       0.619       0.0   0.0        0.0   \n",
       "...                   ...        ...         ...       ...   ...        ...   \n",
       "920200               1.00        0.0       0.378       0.0   0.0        0.0   \n",
       "920201               0.60        0.0       0.241       0.0   0.0        0.0   \n",
       "920202               0.62        0.0       1.492       0.0   0.0        0.0   \n",
       "920203               0.34        0.0       0.952       0.0   0.0        0.0   \n",
       "920204               0.26        0.0       1.297       0.0   0.0        0.0   \n",
       "\n",
       "        target  \n",
       "0            1  \n",
       "1            0  \n",
       "2            0  \n",
       "3            1  \n",
       "4            0  \n",
       "...        ...  \n",
       "920200       0  \n",
       "920201       0  \n",
       "920202       1  \n",
       "920203       0  \n",
       "920204       0  \n",
       "\n",
       "[2960891 rows x 16 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41fcdd32-d019-4c59-b66f-8d92b9b8c055",
   "metadata": {},
   "source": [
    "# Logistic Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fddab2f8-c67d-4885-a61a-c44dd643b8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91   1067054\n",
      "           1       0.86      0.85      0.85    664486\n",
      "\n",
      "    accuracy                           0.89   1731540\n",
      "   macro avg       0.88      0.88      0.88   1731540\n",
      "weighted avg       0.89      0.89      0.89   1731540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# define data\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "def get_folds(folds):\n",
    "    all_folds = []\n",
    "    current = folds\n",
    "    for i in range(0, len(folds)):\n",
    "        all_folds.append(current)\n",
    "        current = rotate(current, 1)\n",
    "    return all_folds\n",
    "\n",
    "def rotate(l, n):\n",
    "    return l[n:] + l[:n]\n",
    "\n",
    "path = \"./data/ml_with_type/2_folds/logistic1\"\n",
    "permutations = get_folds([1, 2, 3, 4, 5])\n",
    "\n",
    "for j, permutation in enumerate(permutations):\n",
    "    permutation = permutations[3]\n",
    "    train = []\n",
    "    for i in range(len(permutation)-1):\n",
    "        fold = permutation[i]\n",
    "        data = pd.read_csv(f\"{path}/fold{fold}.csv\")\n",
    "        train.append(data)\n",
    "\n",
    "    train_fold = pd.concat(train)\n",
    "    train_fold.drop_duplicates(subset=list(set(train_fold.columns) - set([\"tableName\"])), inplace=True)\n",
    "    fold = permutation[-1]\n",
    "    test_fold = pd.read_csv(f\"{path}/fold{fold}.csv\")    \n",
    "    X_train, y_train = train_fold.drop([\"tableName\", \"target\"], axis=1), train_fold[\"target\"]\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    \n",
    "\n",
    "    X_test, y_test = test_fold.drop([\"tableName\", \"target\"], axis=1), test_fold[\"target\"]\n",
    "    scaler = preprocessing.StandardScaler().fit(X_test)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_resampled, y_resampled = (X_train, y_train)\n",
    "    # define oversampling strategy\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "    #X_resampled, y_resampled = oversample.fit_resample(X_train, y_train)\n",
    "    #X_resampled, y_resampled = ADASYN().fit_resample(X_train, y_train)\n",
    "\n",
    "    logmodel = LogisticRegression()\n",
    "    logmodel.fit(X_resampled, y_resampled)\n",
    "    predictions = logmodel.predict(X_test)\n",
    "    print(f\"FOLD\", classification_report(y_test.values,predictions))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6180b0b-60be-45ab-9d69-f8c2381d894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(logmodel, open('logistic1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "cde4ff0c-0026-4202-a58f-4b545ca984a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "def get_folds(folds):\n",
    "    all_folds = []\n",
    "    current = folds\n",
    "    for i in range(0, len(folds)):\n",
    "        all_folds.append(current)\n",
    "        current = rotate(current, 1)\n",
    "    return all_folds\n",
    "\n",
    "def rotate(l, n):\n",
    "    return l[n:] + l[:n]\n",
    "\n",
    "path = \"./data/tmp/\"\n",
    "permutations = get_folds([1, 2, 3, 4, 5])\n",
    "\n",
    "for j, permutation in enumerate(permutations):\n",
    "    permutation = permutations[3]\n",
    "    train = []\n",
    "    for i in range(len(permutation)-1):\n",
    "        fold = permutation[i]\n",
    "        data = pd.read_csv(f\"{path}/fold{fold}.csv\")\n",
    "        train.append(data)\n",
    "\n",
    "    train_fold = pd.concat(train)\n",
    "    train_fold.drop_duplicates(subset=list(set(train_fold.columns) - set([\"target\", \"tableName\"])), inplace=True)\n",
    "    X_train, y_train = train_fold.drop([\"tableName\", \"target\"], axis=1), train_fold[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71ca83b5-8e3b-4120-94ac-64c6561f4874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.422877  , 0.577123  ],\n",
       "       [0.3832703 , 0.6167297 ],\n",
       "       [0.35806257, 0.64193743],\n",
       "       ...,\n",
       "       [0.34667271, 0.65332729],\n",
       "       [0.99878712, 0.00121288],\n",
       "       [0.97780906, 0.02219094]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c1bfb4d3-4ccd-4d8b-8a6c-cd3123152557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddab18d-bb0f-4c45-97d4-088861ef860a",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "b2491cb2-5f0f-4d44-a45a-1f2059e0396f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic2_r1.pkl']"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib_file = \"logistic2_r1.pkl\"\n",
    "joblib.dump(logmodel, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6891b948-b296-4350-ba65-9cb8bae90a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "68ffefa3-490d-49da-b902-f25c121b4354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(y_train),\n",
    "                                        y = y_train                                          \n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f8ae9cc2-d491-4f14-93b1-688a6493fbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76161029, 1.4556199 ])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87547df3-bef2-4991-8d4f-ce272a07c29d",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b37007aa-2431-40bb-bfd1-cbb60f985f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "622/622 [==============================] - 30s 42ms/step - loss: 0.2164 - accuracy: 0.9117\n",
      "Epoch 2/10\n",
      "622/622 [==============================] - 26s 42ms/step - loss: 0.1813 - accuracy: 0.9285\n",
      "Epoch 3/10\n",
      "622/622 [==============================] - 26s 42ms/step - loss: 0.1720 - accuracy: 0.9326\n",
      "Epoch 4/10\n",
      "622/622 [==============================] - 25s 41ms/step - loss: 0.1671 - accuracy: 0.9348\n",
      "Epoch 5/10\n",
      "622/622 [==============================] - 26s 41ms/step - loss: 0.1627 - accuracy: 0.9366\n",
      "Epoch 6/10\n",
      "622/622 [==============================] - 25s 41ms/step - loss: 0.1591 - accuracy: 0.9380\n",
      "Epoch 7/10\n",
      "622/622 [==============================] - 25s 40ms/step - loss: 0.1563 - accuracy: 0.9390\n",
      "Epoch 8/10\n",
      "622/622 [==============================] - 25s 40ms/step - loss: 0.1541 - accuracy: 0.9400\n",
      "Epoch 9/10\n",
      "622/622 [==============================] - 25s 40ms/step - loss: 0.1516 - accuracy: 0.9409\n",
      "Epoch 10/10\n",
      "622/622 [==============================] - 25s 40ms/step - loss: 0.1495 - accuracy: 0.9419\n",
      "150/150 [==============================] - 2s 14ms/step - loss: 0.2866 - accuracy: 0.8947\n",
      "Loss =  0.28662046790122986\n",
      "Accuracy =  0.8947290182113647\n",
      "150/150 [==============================] - 2s 14ms/step\n",
      "FOLD1               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92    932364\n",
      "           1       0.90      0.81      0.85    561742\n",
      "\n",
      "   micro avg       0.89      0.89      0.89   1494106\n",
      "   macro avg       0.90      0.88      0.89   1494106\n",
      "weighted avg       0.90      0.89      0.89   1494106\n",
      " samples avg       0.89      0.89      0.89   1494106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import pandas as pd\n",
    "# first neural network with keras tutorial\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "def get_folds(folds):\n",
    "    all_folds = []\n",
    "    current = folds\n",
    "    for i in range(0, len(folds)):\n",
    "        all_folds.append(current)\n",
    "        current = rotate(current, 1)\n",
    "    return all_folds\n",
    "\n",
    "def rotate(l, n):\n",
    "    return l[n:] + l[:n]\n",
    "\n",
    "\n",
    "path = \"./data/ml_with_type/2_folds/neural1/\"\n",
    "permutations = get_folds([1, 2, 3, 4, 5])\n",
    "permutations\n",
    "#output = open(\"network_0_hidden_layer.txt\", \"w\")\n",
    "for j, permutation in enumerate(permutations):\n",
    "    permutation = permutations[1]\n",
    "    train = []\n",
    "    for i in range(len(permutation)-1):\n",
    "        fold = permutation[i]\n",
    "        data = pd.read_csv(f\"{path}/fold{fold}.csv\")\n",
    "        train.append(data)\n",
    "        \n",
    "    train_fold = pd.concat(train)\n",
    "    train_fold.drop_duplicates(subset=list(set(train_fold.columns) - set([\"tableName\"])), inplace=True)\n",
    "    fold = permutation[-1]\n",
    "    test_fold = pd.read_csv(f\"{path}/fold{fold}.csv\")    \n",
    "    X_train, y_train = train_fold.drop([\"tableName\", \"target\"], axis=1), train_fold[\"target\"]\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight = \"balanced\",\n",
    "        classes = np.unique(y_train),\n",
    "        y = y_train                                          \n",
    "    )\n",
    "    X_test, y_test = test_fold.drop([\"tableName\", \"target\"], axis=1), test_fold[\"target\"]\n",
    "    X_resampled, y_resampled = (X_train, y_train)\n",
    "    #X_resampled, y_resampled = SMOTE().fit_resample(X_train, y_train)\n",
    "    Y_train = np_utils.to_categorical(y_resampled, 2)\n",
    "    Y_test = np_utils.to_categorical(y_test, 2)\n",
    "    # load the dataset\n",
    "    # define the keras model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape=(len(X_train.columns),), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    # compile the keras model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit the keras model on the dataset\n",
    "    model.fit(X_resampled, Y_train, epochs=10, batch_size=10000, class_weight={0:class_weights[0], 1:class_weights[1]})\n",
    "    # Test, Loss and accuracy\n",
    "    loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=10000)\n",
    "    print('Loss = ',loss_and_metrics[0])\n",
    "    print('Accuracy = ',loss_and_metrics[1])\n",
    "    predictions = model.predict(X_test, batch_size=10000)\n",
    "    # Convert the predictions to binary labels\n",
    "    predictions = np.where(predictions > 0.5, 1, 0)\n",
    "\n",
    "    # Calculate the classification report\n",
    "    report = classification_report(Y_test, predictions)\n",
    "\n",
    "    # Print the report\n",
    "    print(f\"FOLD{j+1}\", report)\n",
    "    #output.write(f\"FOLD{j+1} {report}\\n\")\n",
    "    break\n",
    "#output.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4304fe8-be51-459e-b1bf-29a35fca4020",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"neural_network1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "7e904259-78dd-4598-8366-a4a4b31f95b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit([list(test.values())])\n",
    "X_test = scaler.transform([list(test.values())])\n",
    "logmodel.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7c0f82-5e64-48fe-a224-b818cc6e8d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model1 = load_model(\"./process/neural_network2_r1.h5\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "207b3d7f-94a3-4a73-b005-f55012c9a6b9",
   "metadata": {},
   "source": [
    "Epoch 1/10\n",
    "609/609 [==============================] - 27s 39ms/step - loss: 0.2119 - accuracy: 0.9132\n",
    "Epoch 2/10\n",
    "609/609 [==============================] - 24s 39ms/step - loss: 0.1803 - accuracy: 0.9284\n",
    "Epoch 3/10\n",
    "609/609 [==============================] - 24s 39ms/step - loss: 0.1720 - accuracy: 0.9322\n",
    "Epoch 4/10\n",
    "609/609 [==============================] - 24s 39ms/step - loss: 0.1665 - accuracy: 0.9345\n",
    "Epoch 5/10\n",
    "609/609 [==============================] - 24s 39ms/step - loss: 0.1623 - accuracy: 0.9363\n",
    "Epoch 6/10\n",
    "609/609 [==============================] - 24s 39ms/step - loss: 0.1588 - accuracy: 0.9377\n",
    "Epoch 7/10\n",
    "609/609 [==============================] - 24s 39ms/step - loss: 0.1555 - accuracy: 0.9392\n",
    "Epoch 8/10\n",
    "609/609 [==============================] - 24s 39ms/step - loss: 0.1532 - accuracy: 0.9400\n",
    "Epoch 9/10\n",
    "609/609 [==============================] - 24s 39ms/step - loss: 0.1511 - accuracy: 0.9409\n",
    "Epoch 10/10\n",
    "609/609 [==============================] - 24s 39ms/step - loss: 0.1493 - accuracy: 0.9416\n",
    "162/162 [==============================] - 3s 15ms/step - loss: 0.1479 - accuracy: 0.9441\n",
    "Loss =  0.14792127907276154\n",
    "Accuracy =  0.944076657295227\n",
    "162/162 [==============================] - 2s 13ms/step\n",
    "FOLD1               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.95      0.96      0.95    984378\n",
    "           1       0.93      0.92      0.93    627758\n",
    "\n",
    "   micro avg       0.94      0.94      0.94   1612136\n",
    "   macro avg       0.94      0.94      0.94   1612136\n",
    "weighted avg       0.94      0.94      0.94   1612136\n",
    " samples avg       0.94      0.94      0.94   1612136\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fdcf511-765c-46e9-9e6f-be1d44d4d185",
   "metadata": {},
   "source": [
    "Epoch 1/10\n",
    "2151/2151 [==============================] - 104s 46ms/step - loss: 0.1093 - accuracy: 0.9579\n",
    "Epoch 2/10\n",
    "2151/2151 [==============================] - 100s 46ms/step - loss: 0.0872 - accuracy: 0.9675\n",
    "Epoch 3/10\n",
    "2151/2151 [==============================] - 99s 46ms/step - loss: 0.0819 - accuracy: 0.9696\n",
    "Epoch 4/10\n",
    "2151/2151 [==============================] - 100s 47ms/step - loss: 0.0789 - accuracy: 0.9707\n",
    "Epoch 5/10\n",
    "2151/2151 [==============================] - 100s 47ms/step - loss: 0.0767 - accuracy: 0.9716\n",
    "Epoch 6/10\n",
    "2151/2151 [==============================] - 100s 46ms/step - loss: 0.0748 - accuracy: 0.9723\n",
    "Epoch 7/10\n",
    "2151/2151 [==============================] - 100s 46ms/step - loss: 0.0732 - accuracy: 0.9729\n",
    "Epoch 8/10\n",
    "2151/2151 [==============================] - 101s 47ms/step - loss: 0.0719 - accuracy: 0.9734\n",
    "Epoch 9/10\n",
    "2151/2151 [==============================] - 100s 47ms/step - loss: 0.0707 - accuracy: 0.9739\n",
    "Epoch 10/10\n",
    "2151/2151 [==============================] - 100s 47ms/step - loss: 0.0699 - accuracy: 0.9742\n",
    "492/492 [==============================] - 9s 18ms/step - loss: 0.0950 - accuracy: 0.9671\n",
    "Loss =  0.09499507397413254\n",
    "Accuracy =  0.9671008586883545\n",
    "492/492 [==============================] - 7s 13ms/step\n",
    "FOLD1               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.97      0.98   4387657\n",
    "           1       0.78      0.98      0.86    528638\n",
    "\n",
    "   micro avg       0.97      0.97      0.97   4916295\n",
    "   macro avg       0.89      0.97      0.92   4916295\n",
    "weighted avg       0.97      0.97      0.97   4916295\n",
    " samples avg       0.97      0.97      0.97   4916295\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ff79c7a-5e5e-4488-911a-cdfdb731e8e2",
   "metadata": {},
   "source": [
    "5787/5787 [==============================] - 123s 21ms/step - loss: 0.1637 - accuracy: 0.9343\n",
    "1459/1459 [==============================] - 11s 7ms/step - loss: 0.2152 - accuracy: 0.9223\n",
    "Loss =  0.21522372961044312\n",
    "Accuracy =  0.9223216772079468\n",
    "1459/1459 [==============================] - 9s 6ms/step\n",
    "FOLD1               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.91      0.95   1191197\n",
    "           1       0.71      0.96      0.82    267267\n",
    "\n",
    "   micro avg       0.92      0.92      0.92   1458464\n",
    "   macro avg       0.85      0.94      0.89   1458464\n",
    "weighted avg       0.94      0.92      0.93   1458464\n",
    " samples avg       0.92      0.92      0.92   1458464\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5f56c27-403f-4d8d-b0cf-79f8cfa7f6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                960       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64)               256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,130\n",
      "Trainable params: 39,234\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc25648b-c530-4ed2-88b9-9c5104413c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 61ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.06059259, 0.93940747]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = { \n",
    "    \"ntoken\": 2,\n",
    "\"popularity\": 10,\n",
    "\"pos_score\": 0.11,\n",
    "\"es_score\": 27.19,\n",
    "\"es_diff_score\": 1.23E-4,\n",
    "\"ed\": 0.87,\n",
    "\"jaccard\": 0.5,\n",
    "\"jaccardNgram\": 0.8,\n",
    "\"cosine_similarity\": 0.68,\n",
    "\"p_subj_ne\": 0,\n",
    "\"p_subj_lit\": 1.895,\n",
    "\"p_obj_ne\": 0,\n",
    "\"desc\": 0.0,\n",
    "\"descNgram\": 0.0,\n",
    "    \"cta\": 1,\n",
    "    \"ctaMax\": 1,\n",
    "    \"cpa\": 1,\n",
    "    \"cpaMax\": 1,\n",
    "    \"neural1_score\": 0,\n",
    "    \"neural1_score_diff\": 0\n",
    "}\n",
    "model.predict([list(test.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "0069d074-f455-4219-bb3b-24de6b86faf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 61ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.5316918e-06, 9.9999046e-01]], dtype=float32)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = { \n",
    "    \"ntoken\" : 4, \n",
    "    \"popularity\" : 30, \n",
    "    \"pos_score\" : 0.01, \n",
    "    \"es_score\" : 105.96, \n",
    "    \"es_diff_score\" : 1, \n",
    "    \"ed\" : 0.68, \n",
    "    \"jaccard\" : 0.75, \n",
    "    \"jaccardNgram\" : 0.61, \n",
    "    \"cosine_similarity\" : 0.87, \n",
    "    \"p_subj_ne\" : 2.0, \n",
    "    \"p_subj_lit\" : 0, \n",
    "    \"p_obj_ne\" : 0, \n",
    "    \"desc\" : 0.0, \n",
    "    \"descNgram\" : 0.5\n",
    "}\n",
    "model.predict([list(test.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "b640ff70-bc52-482f-b0ce-21bd46a3bfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 113ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.06914807, 0.930852  ]], dtype=float32)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = {\n",
    "\"ntoken\": 2,\n",
    "\"popularity\": 10,\n",
    "\"pos_score\": 0.11,\n",
    "\"es_score\": 27.19,\n",
    "\"es_diff_score\": 1.23E-4,\n",
    "\"ed\": 0.87,\n",
    "\"jaccard\": 0.5,\n",
    "\"jaccardNgram\": 0.8,\n",
    "\"cosine_similarity\": 0.68,\n",
    "\"p_subj_ne\": 0,\n",
    "\"p_subj_lit\": 1.895,\n",
    "\"p_obj_ne\": 0,\n",
    "\"desc\": 0.0,\n",
    "\"descNgram\": 0.0\n",
    "}\n",
    "model.predict([list(test.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "4912be7c-0073-4297-bf4b-17438d3ab97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 70ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[8.321295e-08, 9.999999e-01]], dtype=float32)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = { \n",
    "  \"ntoken\": 2,\n",
    "\"popularity\": 1,\n",
    "\"pos_score\": 0.01,\n",
    "\"es_score\": 58.34,\n",
    "\"es_diff_score\": 1.112493,\n",
    "\"ed\": 1.0,\n",
    "\"jaccard\": 1.0,\n",
    "\"jaccardNgram\": 1.0,\n",
    "\"cosine_similarity\": 1.0,\n",
    "\"p_subj_ne\": 0,\n",
    "\"p_subj_lit\": 2.283,\n",
    "\"p_obj_ne\": 0,\n",
    "\"desc\": 0.0,\n",
    "\"descNgram\": 0.0,\n",
    "}\n",
    "model.predict([list(test.values())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233cac32-67de-4b9f-843f-9dc954ef1b72",
   "metadata": {},
   "source": [
    "# AVG perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc0d2663-4cb0-4f89-89da-ed9b1fba7a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network_0_hidden_layer.txt \n",
      " \t\tP \t\tR \t\tF1\n",
      "0 \t0.95 \t0.94 \t0.94\n",
      "1 \t0.88 \t0.89 \t0.89 \n",
      "\n",
      "network_1_hidden_layer.txt \n",
      " \t\tP \t\tR \t\tF1\n",
      "0 \t0.95 \t0.94 \t0.95\n",
      "1 \t0.89 \t0.9 \t0.89 \n",
      "\n",
      "logistic.txt \n",
      " \t\tP \t\tR \t\tF1\n",
      "0 \t0.96 \t0.88 \t0.92\n",
      "1 \t0.79 \t0.93 \t0.86 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in [\"./network_0_hidden_layer.txt\", \"network_1_hidden_layer.txt\", \"logistic.txt\"]:\n",
    "    file = open(file)\n",
    "    data = file.read().split(\"\\n\")\n",
    "    data[0].split()\n",
    "    c0 = [0, 0, 0]\n",
    "    c1 = [0, 0, 0]\n",
    "    for row in data:\n",
    "        row = row.split()\n",
    "        if len(row) == 0:\n",
    "            continue\n",
    "        if row[0] == '0':\n",
    "            for i, v in enumerate(row[1:4]):\n",
    "                c0[i] += float(v)\n",
    "        elif row[0] == '1':\n",
    "            for i, v in enumerate(row[1:4]):\n",
    "                c1[i] += float(v)        \n",
    "\n",
    "\n",
    "    header = [\"\\t\\tP\", \"\\t\\tR\", \"\\t\\tF1\"]\n",
    "    c0 = [\"0\"] + [f\"\\t{round(v/5, 2)}\" for v in c0] \n",
    "    c1 = [\"1\"] + [f\"\\t{round(v/5, 2)}\" for v in c1]    \n",
    "    out = \" \".join(header) + \"\\n\" + \" \".join(c0) + \"\\n\" +\" \".join(c1)\n",
    "    print(file.name.split(\"/\")[-1], \"\\n\", out, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55b567-fb09-496b-af92-65b13781c834",
   "metadata": {},
   "source": [
    "# Score Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f6e4e1c2-b6d1-4f24-bdc1-237c7a2d707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def make_prediction(model, data_source):\n",
    "    test = pd.read_csv(data_source)\n",
    "    new_df = test.drop(columns=[\"tableName\", \"target\", \"cea\"])\n",
    "    predictions = model.predict(new_df, batch_size=1000)\n",
    "    y_classes = predictions.argmax(axis=-1)\n",
    "    out = []\n",
    "    for i, prediciton in enumerate(predictions):\n",
    "        outcome = \"correct\" if y_classes[i] == test[\"target\"][i] else \"incorrect\"\n",
    "        out.append([y_classes[i], test[\"target\"][i], prediciton[y_classes[i]], outcome])\n",
    "    out_df = pd.DataFrame(out, columns=[\"prediciton\", \"gt\", \"score\", \"outcome\"])\n",
    "    \n",
    "    return out_df\n",
    "\n",
    "\n",
    "def plot_data(data):\n",
    "    for i, out in enumerate(data):\n",
    "        x1 = out[out[\"outcome\"] == \"correct\"][\"score\"]\n",
    "        x2 = out[out[\"outcome\"] == \"incorrect\"][\"score\"]\n",
    "\n",
    "\n",
    "        # Assign colors for each airline and the names\n",
    "        colors = ['GREEN', 'RED']\n",
    "        names = ['correct', 'incorrect']\n",
    "        f = plt.figure(figsize=(50,20))\n",
    "        ax = plt.subplot(2, 3, i+1)\n",
    "        # Make the histogram using a list of lists\n",
    "        # Normalize the flights and assign colors and names\n",
    "        ax.hist([x1, x2], bins = int(100), color = colors, label=names)\n",
    "\n",
    "        # Plot formatting\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('Prediction Value Scores')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title(f'Precision by confidence on')\n",
    "        #plt.savefig(f'./data/scores2/{.png', bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9d46f-3bf4-4faa-a7a1-0a6a142b77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "#out = pd.DataFrame(header=[\"prediciton\", \"gt\", \"score\", \"outcome\"])\n",
    "for i, prediciton in enumerate(predictions):\n",
    "    outcome = \"correct\" if y_classes[i] == test[\"target\"][i] else \"incorrect\"\n",
    "    out.append([y_classes[i], test[\"target\"][i], prediciton[y_classes[i]], outcome])\n",
    "out_df = pd.DataFrame(out, columns=[\"prediciton\", \"gt\", \"score\", \"outcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "9baaf829-8ccf-4f90-b6ba-8c0b1cf108a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3600x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df\n",
    "x1 = df[df[\"outcome\"] == \"correct\"][\"score\"]\n",
    "x2 = df[df[\"outcome\"] == \"incorrect\"][\"score\"]\n",
    "\n",
    "\n",
    "# Assign colors for each airline and the names\n",
    "colors = ['GREEN', 'RED']\n",
    "names = ['correct', 'incorrect']\n",
    "f = plt.figure(figsize=(50,20))\n",
    "ax = plt.subplot(2, 3, 1)\n",
    "# Make the histogram using a list of lists\n",
    "# Normalize the flights and assign colors and names\n",
    "ax.hist([x1, x2], bins = int(100), color = colors, label=names)\n",
    "\n",
    "# Plot formatting\n",
    "ax.legend()\n",
    "ax.set_xlabel('Prediction Value Scores')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title(f'Precision by confidence on')\n",
    "#plt.savefig(f'./data/scores2/{.png', bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
